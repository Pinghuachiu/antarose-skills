#!/usr/bin/env python3
"""
Social Content Writer - Pain-Point Based Hook Generation Script
åŸºæ–¼çœŸæ­£ç—›é»ç”Ÿæˆå‹¾å­ï¼ˆæ•´åˆ researchï¼‰
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import List, Dict
import subprocess
import re

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


# ç—›é»é¡å‹å®šç¾©
PAIN_POINT_TYPES = {
    "time": {
        "name": "æ™‚é–“ç—›é»",
        "description": "æµªè²»æ™‚é–“ã€æ•ˆç‡ä½ã€æ™‚é–“ä¸å¤ ç”¨",
        "keywords": ["èŠ±æ™‚é–“", "æµªè²»", "æ•ˆç‡", "æ™‚é–“", "å°æ™‚", "åˆ†é˜"],
        "hook_templates": [
            "æ¯å¤©èŠ±{time}è™•ç†{task}ï¼Ÿ{solution}å¯ä»¥å¹«ä½ çœå›ä¾†",
            "ä½ é‚„åœ¨æ‰‹å‹•{task}ï¼Ÿåˆ¥äººå·²ç¶“ç”¨{solution}è‡ªå‹•åŒ–äº†",
            "{time}çš„ç‘£äº‹ï¼Œ{solution}åœ¨ä½ ç¡è¦ºæ™‚å¹«ä½ åšå®Œäº†",
            "æƒ³åƒä¸€ä¸‹ï¼šå¦‚æœæ¯å¤©å¤šå‡º{time}ï¼Œä½ èƒ½åšä»€éº¼ï¼Ÿ"
        ]
    },
    "money": {
        "name": "é‡‘éŒ¢ç—›é»",
        "description": "èŠ±è²»å¤ªé«˜ã€æˆæœ¬å¤±æ§ã€ä¸åˆ’ç®—",
        "keywords": ["èŠ±è²»", "æˆæœ¬", "éŒ¢", "è²´", "ä¾¿å®œ", "æœˆè–ª", "å¹´è–ª"],
        "hook_templates": [
            "è˜è«‹åŠ©ç†æœˆè–ª${money}ï¼Ÿ{solution}å…è²»é‚„æ›´å¥½ç”¨",
            "å·²ç¶“å¹«{number}äººçœäº†${money}ï¼Œä½ ä¹Ÿå¯ä»¥",
            "ç‚ºä»€éº¼ä»˜éŒ¢åš{task}ï¼Ÿ{solution}å¹«ä½ å…è²»åš",
            "æŠ•è³‡å›å ±ç‡{roi}%ï¼š{solution}å€¼å¾—å—ï¼Ÿ"
        ]
    },
    "effort": {
        "name": "åŠªåŠ›ç—›é»",
        "description": "å¤ªç´¯ã€å¤ªè¤‡é›œã€å­¸ä¸æœƒ",
        "keywords": ["ç´¯", "è¤‡é›œ", "é›£", "éº»ç…©", "å­¸ä¸æœƒ", "æä¸æ‡‚"],
        "hook_templates": [
            "{task}å¤ªç´¯äº†ï¼Ÿ{solution}è®“ä½ ä¸€åˆ†é˜æå®š",
            "ä½ ä¸æ˜¯ä¸æœƒï¼Œåªæ˜¯é‚„æ²’ç”¨å°å·¥å…·",
            "åˆ¥äºº{time}å­¸æœƒï¼Œä½ {time}å°±èƒ½ä¸Šæ‰‹",
            "{number}æ­¥è®Š{steps}æ­¥ï¼š{solution}ç°¡åŒ–äº†{task}"
        ]
    },
    "error": {
        "name": "éŒ¯èª¤ç—›é»",
        "description": "å®¹æ˜“å‡ºéŒ¯ã€éºæ¼ã€å¤±æ•—",
        "keywords": ["éŒ¯èª¤", "å¤±æ•—", "éºæ¼", "å¿˜è¨˜", "å‡ºéŒ¯", "æç ¸"],
        "hook_templates": [
            "å†ä¹Ÿä¸æœƒ{mistake}äº†ï¼Œ{solution}å¹«ä½ è¨˜ä½",
            "{number}%çš„äººéƒ½çŠ¯é{mistake}ï¼Œä½ ä¹Ÿæ˜¯å—ï¼Ÿ",
            "å› ç‚º{mistake}æå¤±${money}ï¼Ÿ{solution}å¹«ä½ é¿å…",
            "ä¸€æ¬¡éŒ¯éƒ½ä¸èƒ½çŠ¯ï¼Ÿ{solution}è®“ä½ é›¶å¤±èª¤"
        ]
    },
    "competition": {
        "name": "ç«¶çˆ­ç—›é»",
        "description": "è½å¾Œæ–¼äººã€è¼¸åœ¨èµ·è·‘ç·š",
        "keywords": ["è½å¾Œ", "è¼¸", "ç«¶çˆ­", "å°æ‰‹", "å„ªå‹¢", "æ¶å…ˆ"],
        "hook_templates": [
            "åˆ¥äººå·²ç¶“åœ¨ç”¨{solution}ï¼Œä½ é‚„åœ¨ç­‰ä»€éº¼ï¼Ÿ",
            "å†ä¸{action}ï¼Œå°±çœŸçš„è¼¸äº†",
            "{competitor}éƒ½åœ¨ç”¨ï¼Œä½ é‚„ä¸çŸ¥é“ï¼Ÿ",
            "æ¶å…ˆä¸€æ­¥ï¼š{solution}è®“ä½ é ˜å…ˆ{competitor}"
        ]
    },
    "fear": {
        "name": "ææ‡¼ç—›é»",
        "description": "å®³æ€•è¢«æ·˜æ±°ã€å®³æ€•éŒ¯å¤±",
        "keywords": ["æ·˜æ±°", "éŒ¯å¤±", "å®³æ€•", "æ“”å¿ƒ", "é¢¨éšª", "å±éšª"],
        "hook_templates": [
            "å†ä¸{action}ï¼Œå°±è¢«æ·˜æ±°äº†",
            "å®³æ€•éŒ¯é{opportunity}ï¼Ÿ{solution}å¹«ä½ æŠ“ä½",
            "{number}%çš„äººå·²ç¶“é–‹å§‹{action}ï¼Œä½ é‚„åœ¨ç­‰",
            "é€™å¯èƒ½æ˜¯ä½ æœ€å¾Œçš„æ©Ÿæœƒ"
        ]
    }
}


class PainPointHookGenerator:
    """åŸºæ–¼ç—›é»çš„å‹¾å­ç”Ÿæˆå™¨"""

    def __init__(self, platform: str = "facebook"):
        self.platform = platform
        self.platform_limits = self._get_platform_limits()

    def _get_platform_limits(self) -> Dict:
        """ç²å–å¹³å°é™åˆ¶"""
        limits = {
            "facebook": {"max_length": 60_000, "optimal_length": 80},
            "instagram": {"max_length": 2_200, "optimal_length": 40},
            "threads": {"max_length": 500, "optimal_length": 50},
            "linkedin": {"max_length": 3_000, "optimal_length": 100}
        }
        return limits.get(self.platform, {"max_length": 10_000, "optimal_length": 80})

    def analyze_pain_points_from_research(self, research_data: Dict) -> List[Dict]:
        """å¾ç ”ç©¶è³‡æ–™ä¸­åˆ†æç—›é»"""
        print("\nğŸ” åˆ†æç—›é»ä¸­...")

        pain_points = []

        # æ”¶é›†æ‰€æœ‰å…§å®¹æ‘˜è¦
        all_summaries = []
        for item in research_data.get("data", []):
            summary = item.get("summary", "")
            if summary:
                all_summaries.append(summary)

        combined_text = " ".join(all_summaries)

        # åˆ†ææ¯ç¨®ç—›é»é¡å‹
        for pain_type, config in PAIN_POINT_TYPES.items():
            # æª¢æŸ¥é—œéµè©å‡ºç¾é »ç‡
            keywords = config["keywords"]
            keyword_count = sum(combined_text.lower().count(kw.lower()) for kw in keywords)

            if keyword_count > 0:
                # æå–ç›¸é—œå¥å­
                relevant_sentences = self._extract_relevant_sentences(combined_text, keywords)

                pain_points.append({
                    "type": pain_type,
                    "name": config["name"],
                    "description": config["description"],
                    "keyword_count": keyword_count,
                    "relevant_sentences": relevant_sentences[:3],  # å‰ 3 å€‹ç›¸é—œå¥å­
                    "severity": min(1.0, keyword_count / 10)  # åš´é‡ç¨‹åº¦
                })

        # æŒ‰åš´é‡ç¨‹åº¦æ’åº
        pain_points.sort(key=lambda x: x["severity"], reverse=True)

        print(f"âœ… ç™¼ç¾ {len(pain_points)} ç¨®ç—›é»")

        for pp in pain_points:
            print(f"   â€¢ {pp['name']}: åš´é‡åº¦ {pp['severity']:.2f}")

        return pain_points

    def _extract_relevant_sentences(self, text: str, keywords: List[str]) -> List[str]:
        """æå–åŒ…å«é—œéµè©çš„å¥å­"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        relevant = []

        for sentence in sentences:
            if len(sentence) > 10:
                for kw in keywords:
                    if kw.lower() in sentence.lower():
                        relevant.append(sentence.strip())
                        break

        return relevant

    def generate_hooks_from_pain_points(self, topic: str, pain_points: List[Dict],
                                         num_hooks: int = 10) -> List[Dict]:
        """å¾ç—›é»ç”Ÿæˆå‹¾å­"""
        print(f"\nâœ¨ ç”Ÿæˆ {num_hooks} å€‹å‹¾å­...")

        hooks = []
        hooks_per_type = max(1, num_hooks // len(pain_points)) if pain_points else num_hooks

        for pain_point in pain_points[:5]:  # æœ€å¤šè™•ç†å‰ 5 å€‹ç—›é»
            pain_type = pain_point["type"]
            config = PAIN_POINT_TYPES[pain_type]
            templates = config["hook_templates"]

            # å¾ç›¸é—œå¥å­ä¸­æå–å¯¦ä¾‹
            examples = pain_point.get("relevant_sentences", [])

            for i in range(min(hooks_per_type, len(templates))):
                template = templates[i % len(templates)]

                # å¡«å……æ¨¡æ¿
                hook_text = self._fill_hook_template(template, topic, pain_point, examples)

                # è¨ˆç®—æ•ˆæœåˆ†æ•¸ï¼ˆåŸºæ–¼ç—›é»åš´é‡ç¨‹åº¦ï¼‰
                base_score = 70 + (pain_point["severity"] * 20)
                effectiveness_score = round(min(95, base_score), 0)

                hook = {
                    "type": pain_type,
                    "pain_point": config["name"],
                    "text": hook_text,
                    "effectiveness_score": int(effectiveness_score),
                    "length": len(hook_text),
                    "platform": self.platform,
                    "based_on_research": True
                }
                hooks.append(hook)

                if len(hooks) >= num_hooks:
                    break

            if len(hooks) >= num_hooks:
                break

        # å¦‚æœé‚„ä¸å¤ ï¼Œè£œå……é€šç”¨å‹¾å­
        if len(hooks) < num_hooks:
            generic_hooks = self._generate_generic_hooks(topic, num_hooks - len(hooks))
            hooks.extend(generic_hooks)

        # æŒ‰æ•ˆæœåˆ†æ•¸æ’åº
        hooks.sort(key=lambda x: x["effectiveness_score"], reverse=True)

        return hooks[:num_hooks]

    def _fill_hook_template(self, template: str, topic: str, pain_point: Dict,
                           examples: List[str]) -> str:
        """å¡«å……å‹¾å­æ¨¡æ¿"""
        result = template

        # å¾ä¾‹å­ä¸­æå–å¯¦éš›æ•¸å€¼
        import re

        # æå–æ™‚é–“ç›¸é—œ
        time_match = re.search(r'(\d+)\s*(å°æ™‚|åˆ†é˜|å°æ™‚)', " ".join(examples))
        time_value = time_match.group(0) if time_match else "2 å°æ™‚"

        # æå–é‡‘éŒ¢ç›¸é—œ
        money_match = re.search(r'\$?(\d+[,\d]*)', " ".join(examples))
        money_value = f"${money_match.group(1)}" if money_match else "$3,000"

        # æå–ç™¾åˆ†æ¯”
        percent_match = re.search(r'(\d+)%', " ".join(examples))
        percent_value = percent_match.group(1) if percent_match else "50"

        # æå–ä»»å‹™æè¿°
        task = "è™•ç†ç‘£äº‹"
        if examples:
            # æ‰¾æœ€å¸¸è¦‹çš„å‹•ä½œè©
            for keyword in ["è™•ç†", "æ•´ç†", "å›è¦†", "ç®¡ç†", "è¨˜éŒ„", "æ§åˆ¶"]:
                if any(keyword in ex for ex in examples):
                    task = keyword
                    break

        # æ›¿æ›ä½”ä½ç¬¦
        replacements = {
            "{time}": time_value,
            "{money}": money_value,
            "{number}": percent_value,
            "{task}": task,
            "{solution}": topic,
            "{action}": "ä½¿ç”¨å®ƒ",
            "{competitor}": "ç«¶çˆ­å°æ‰‹",
            "{opportunity}": "æ©Ÿæœƒ",
            "{mistake}": "éŒ¯èª¤",
            "{roi}": percent_value,
            "{steps}": "3"
        }

        for placeholder, value in replacements.items():
            result = result.replace(placeholder, str(value))

        return result

    def _generate_generic_hooks(self, topic: str, count: int) -> List[Dict]:
        """ç”Ÿæˆé€šç”¨å‹¾å­ï¼ˆç•¶æ²’æœ‰è¶³å¤ ç ”ç©¶è³‡æ–™æ™‚ï¼‰"""
        generic_templates = [
            f"ç‚ºä»€éº¼å¤§å®¶éƒ½åœ¨è¨è«–{topic}ï¼Ÿ",
            f"{topic}å¯èƒ½æœƒæ”¹è®Šä½ çš„å·¥ä½œæ–¹å¼",
            f"å¦‚æœä½ é‚„æ²’ç”¨{topic}ï¼Œç¾åœ¨å¯èƒ½æ˜¯æ™‚å€™äº†",
            f"é€™å°±æ˜¯{topic}ï¼šç°¡å–®ã€å¼·å¤§ã€å…è²»",
            f"é—œæ–¼{topic}ï¼Œä½ éœ€è¦çŸ¥é“çš„å¹¾ä»¶äº‹"
        ]

        hooks = []
        for i in range(min(count, len(generic_templates))):
            hook = {
                "type": "generic",
                "pain_point": "é€šç”¨",
                "text": generic_templates[i],
                "effectiveness_score": 70,
                "length": len(generic_templates[i]),
                "platform": self.platform,
                "based_on_research": False
            }
            hooks.append(hook)

        return hooks

    def generate(self, topic: str, research_file: str = None,
                num_hooks: int = 10) -> List[Dict]:
        """ç”Ÿæˆå‹¾å­ï¼ˆæ•´åˆç ”ç©¶è³‡æ–™ï¼‰"""

        print("\n" + "="*60)
        print("âœ¨ å‹¾å­ç”Ÿæˆå™¨ï¼ˆç—›é»é©…å‹•ï¼‰")
        print("="*60)

        # éšæ®µ 1: å¦‚æœæœ‰ç ”ç©¶è³‡æ–™ï¼Œå…ˆåˆ†æç—›é»
        pain_points = []
        if research_file and os.path.exists(research_file):
            print(f"\nğŸ“‚ è®€å–ç ”ç©¶è³‡æ–™: {research_file}")
            with open(research_file, 'r', encoding='utf-8') as f:
                research_data = json.load(f)

            pain_points = self.analyze_pain_points_from_research(research_data)

        # éšæ®µ 2: å¾ç—›é»ç”Ÿæˆå‹¾å­
        if pain_points:
            hooks = self.generate_hooks_from_pain_points(topic, pain_points, num_hooks)
        else:
            print("\nâš ï¸  æ²’æœ‰ç ”ç©¶è³‡æ–™ï¼Œä½¿ç”¨é€šç”¨å‹¾å­")
            hooks = self._generate_generic_hooks(topic, num_hooks)

        return hooks

    def print_hooks(self, hooks: List[Dict]):
        """æ‰“å°å‹¾å­åˆ—è¡¨"""
        print("\n" + "="*60)
        print("âœ¨ ç”Ÿæˆçš„å‹¾å­ï¼ˆç—›é»é©…å‹•ï¼‰")
        print("="*60)

        for i, hook in enumerate(hooks, 1):
            research_tag = "ğŸ”¬" if hook.get("based_on_research") else "ğŸ“"
            print(f"\n{research_tag} [{i}] {hook['pain_point'].upper()} (åˆ†æ•¸: {hook['effectiveness_score']}/100)")
            print(f"   {hook['text']}")
            print(f"   é•·åº¦: {hook['length']} å­—")

        print("="*60 + "\n")

    def save_to_file(self, hooks: List[Dict], filepath: str):
        """ä¿å­˜å‹¾å­åˆ°æ–‡ä»¶"""
        output = {
            "generated_at": datetime.now().isoformat(),
            "platform": self.platform,
            "total_hooks": len(hooks),
            "hooks": hooks
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"âœ… å‹¾å­å·²ä¿å­˜åˆ°: {filepath}")


def main():
    parser = argparse.ArgumentParser(
        description="åŸºæ–¼ç—›é»ç”Ÿæˆå‹¾å­ï¼ˆæ•´åˆ researchï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # åŸºæ–¼ç ”ç©¶è³‡æ–™ç”Ÿæˆå‹¾å­
  python3 hook-generator.py --topic "Moltbot" --research research_data.json

  # ç¨ç«‹ä½¿ç”¨ï¼ˆä¸ä¾è³´ç ”ç©¶è³‡æ–™ï¼‰
  python3 hook-generator.py --topic "AIå·¥å…·" --num-hooks 10

  # å…ˆåšç ”ç©¶å†ç”Ÿæˆå‹¾å­
  python3 collect.py --topic "Moltbot" --deep-research --output research.json
  python3 hook-generator.py --topic "Moltbot" --research research.json
        """
    )
    parser.add_argument("--topic", required=True, help="ä¸»é¡Œ")
    parser.add_argument("--platform", default="facebook",
                       choices=["facebook", "instagram", "threads", "linkedin"],
                       help="ç›®æ¨™å¹³å°")
    parser.add_argument("--num-hooks", type=int, default=10,
                       help="ç”Ÿæˆå‹¾å­æ•¸é‡")
    parser.add_argument("--research", help="ç ”ç©¶è³‡æ–™æª”æ¡ˆè·¯å¾‘ï¼ˆJSONï¼‰")
    parser.add_argument("--output", default="hooks.json",
                       help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()

    # å‰µå»ºç”Ÿæˆå™¨
    generator = PainPointHookGenerator(platform=args.platform)

    # ç”Ÿæˆå‹¾å­
    hooks = generator.generate(
        topic=args.topic,
        research_file=args.research,
        num_hooks=args.num_hooks
    )

    # æ‰“å°çµæœ
    generator.print_hooks(hooks)

    # ä¿å­˜åˆ°æ–‡ä»¶
    generator.save_to_file(hooks, args.output)


if __name__ == "__main__":
    main()
