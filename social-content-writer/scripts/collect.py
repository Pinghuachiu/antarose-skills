#!/usr/bin/env python3
"""
Social Content Writer - Research Data Collection Script
å¾å¤šå€‹ä¾†æºæ”¶é›†ç›¸é—œè³‡æ–™ä¸¦è©•åˆ†ï¼ˆçœŸæ­£æ•´åˆ MCPï¼‰
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import List, Dict, Any
import subprocess

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class ResearchCollector:
    """ç ”ç©¶è³‡æ–™æ”¶é›†å™¨ - çœŸæ­£æ•´åˆ MCP å·¥å…·"""

    def __init__(self, min_score: float = 0.6):
        self.min_score = min_score
        self.collected_data = []

    def collect_from_web_search(self, topic: str, max_results: int = 10) -> List[Dict]:
        """å¾ç¶²è·¯æœå°‹æ”¶é›†è³‡æ–™ï¼ˆä½¿ç”¨ web-search-prime MCPï¼‰"""
        print(f"ğŸ” æ­£åœ¨æœå°‹ä¸»é¡Œ: {topic}")

        # èª¿ç”¨ web-search-prime MCP
        try:
            # ä½¿ç”¨ subprocess èª¿ç”¨ MCP å·¥å…·ï¼ˆé€šéè‡¨æ™‚ Python è…³æœ¬ï¼‰
            script_content = f'''
import json
import sys
sys.path.insert(0, "/home/jackalchiu/claude/.claude")

from mcp__web_search_prime import webSearchPrime

results = webSearchPrime(
    search_query="{topic}",
    search_recency_filter="oneMonth",
    content_size="high",
    location="cn"
)

print(json.dumps(results))
'''

            # å¯«å…¥è‡¨æ™‚è…³æœ¬
            temp_script = "/tmp/mcp_search.py"
            with open(temp_script, 'w') as f:
                f.write(script_content)

            # åŸ·è¡Œ
            result = subprocess.run(
                ["python3", temp_script],
                capture_output=True,
                text=True,
                timeout=60
            )

            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            os.remove(temp_script)

            if result.returncode == 0 and result.stdout:
                search_results = json.loads(result.stdout)

                if search_results and len(search_results) > 0:
                    processed_results = []
                    for item in search_results[:max_results]:
                        processed = {
                            "source_type": "web_search",
                            "title": item.get("title", ""),
                            "url": item.get("link", ""),
                            "summary": item.get("content", "")[:500],
                            "relevance_score": 0.85,  # æœå°‹çµæœç›¸é—œæ€§é«˜
                            "credibility_score": 0.75,
                            "recency_score": 0.90,
                            "completeness_score": 0.70
                        }
                        processed_results.append(processed)

                    print(f"âœ… æ‰¾åˆ° {len(processed_results)} ç­†æœå°‹çµæœ")
                    return processed_results
                else:
                    print("âš ï¸  æœå°‹çµæœç‚ºç©º")
                    return []
            else:
                print(f"âš ï¸  æœå°‹å¤±æ•—: {result.stderr}")
                return []

        except Exception as e:
            print(f"âš ï¸  æœå°‹éç¨‹å‡ºéŒ¯: {e}")
            return []

    def collect_from_web_reader(self, urls: List[str]) -> List[Dict]:
        """ä½¿ç”¨ web-reader MCP è®€å–ç¶²é å…§å®¹"""
        print(f"ğŸ“– æ­£åœ¨è®€å– {len(urls)} å€‹ç¶²é ...")

        results = []

        for url in urls[:5]:  # é™åˆ¶æ•¸é‡é¿å…è¶…æ™‚
            try:
                script_content = f'''
import json
import sys
sys.path.insert(0, "/home/jackalchiu/claude/.claude")

from mcp__web_reader import webReader

result = webReader(
    url="{url}",
    return_format="markdown",
    retain_images=False
)

print(json.dumps({{"url": "{url}", "content": result}}))
'''

                temp_script = "/tmp/mcp_read.py"
                with open(temp_script, 'w') as f:
                    f.write(script_content)

                result = subprocess.run(
                    ["python3", temp_script],
                    capture_output=True,
                    text=True,
                    timeout=30
                )

                os.remove(temp_script)

                if result.returncode == 0 and result.stdout:
                    data = json.loads(result.stdout)
                    content = data.get("content", "")

                    if content and len(content) > 100:
                        # æå–æ‘˜è¦ï¼ˆå‰ 500 å­—ï¼‰
                        summary = content[:500].replace('\n', ' ')

                        processed = {
                            "source_type": "web_reader",
                            "title": f"å…§å®¹æå–: {url[:50]}...",
                            "url": url,
                            "summary": summary,
                            "relevance_score": 0.90,
                            "credibility_score": 0.85,
                            "recency_score": 0.80,
                            "completeness_score": 0.90
                        }
                        results.append(processed)
                        print(f"  âœ… å·²è®€å–: {url[:50]}...")

            except Exception as e:
                print(f"  âš ï¸  è®€å–å¤±æ•— {url}: {e}")
                continue

        return results

    def collect_from_database(self, topic: str, max_results: int = 5) -> List[Dict]:
        """å¾è³‡æ–™åº«æ”¶é›†æ­·å²è³‡æ–™"""
        print(f"ğŸ’¾ æ­£åœ¨æŸ¥è©¢è³‡æ–™åº«: {topic}")

        # æª¢æŸ¥ MySQL æ˜¯å¦é…ç½®
        if not os.environ.get("MYSQL_HOST"):
            print("  âš ï¸  æœªé…ç½® MySQLï¼Œè·³éè³‡æ–™åº«æŸ¥è©¢")
            return []

        try:
            mysql_script = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                "mysql",
                "scripts",
                "query.py"
            )

            if not os.path.exists(mysql_script):
                print("  âš ï¸  MySQL è…³æœ¬ä¸å­˜åœ¨")
                return []

            # æ§‹å»ºæŸ¥è©¢
            query = f"""
            SELECT topic, content, platform, created_at
            FROM content_history
            WHERE topic LIKE '%{topic}%'
            ORDER BY created_at DESC
            LIMIT {max_results}
            """

            result = subprocess.run(
                ["python3", mysql_script, query],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                data = json.loads(result.stdout)
                results = []
                for item in data:
                    results.append({
                        "source_type": "database",
                        "title": f"æ­·å²å…§å®¹: {item.get('topic', '')}",
                        "url": None,
                        "summary": item.get('content', '')[:300],
                        "relevance_score": 0.80,
                        "credibility_score": 0.90,
                        "recency_score": 0.60,
                        "completeness_score": 0.85
                    })
                print(f"  âœ… æ‰¾åˆ° {len(results)} ç­†æ­·å²è³‡æ–™")
                return results
            else:
                print(f"  âš ï¸  æŸ¥è©¢å¤±æ•—: {result.stderr}")
                return []

        except Exception as e:
            print(f"  âš ï¸  è³‡æ–™åº«æŸ¥è©¢éŒ¯èª¤: {e}")
            return []

    def calculate_quality_score(self, item: Dict) -> float:
        """è¨ˆç®—ç¶œåˆè³ªé‡åˆ†æ•¸"""
        weights = {
            "relevance": 0.4,
            "credibility": 0.3,
            "recency": 0.2,
            "completeness": 0.1
        }

        score = (
            item["relevance_score"] * weights["relevance"] +
            item["credibility_score"] * weights["credibility"] +
            item["recency_score"] * weights["recency"] +
            item["completeness_score"] * weights["completeness"]
        )

        return round(score, 2)

    def extract_key_insights(self, data: List[Dict]) -> List[str]:
        """å¾æ”¶é›†çš„è³‡æ–™ä¸­æå–é—œéµæ´å¯Ÿ"""
        insights = []

        for item in data:
            summary = item.get("summary", "")
            if len(summary) > 50:
                insights.append(summary)

        return insights[:5]  # è¿”å›å‰ 5 å€‹

    def collect(self, topic: str, sources: str = "web_search,web_reader",
                max_results: int = 20, deep_research: bool = False) -> Dict:
        """æ”¶é›†æ‰€æœ‰è³‡æ–™ä¾†æº - æ”¹é€²ç‰ˆ"""

        source_list = [s.strip() for s in sources.split(",")]
        all_data = []
        urls_to_read = []

        print("\n" + "="*60)
        print("ğŸ“š é–‹å§‹è³‡æ–™æ”¶é›†")
        print("="*60 + "\n")

        # éšæ®µ 1: ç¶²è·¯æœå°‹
        if "web_search" in source_list:
            search_results = self.collect_from_web_search(topic, max_results)
            all_data.extend(search_results)

            # æ”¶é›† URL ç”¨æ–¼æ·±å…¥é–±è®€
            if deep_research and "web_reader" in source_list:
                urls_to_read = [item["url"] for item in search_results[:5] if item.get("url")]

        # éšæ®µ 2: æ·±å…¥é–±è®€ç¶²é å…§å®¹
        if deep_research and urls_to_read and "web_reader" in source_list:
            print("\nğŸ“– æ·±å…¥é–±è®€ç¶²é å…§å®¹...")
            reader_results = self.collect_from_web_reader(urls_to_read)
            all_data.extend(reader_results)

        # éšæ®µ 3: è³‡æ–™åº«æŸ¥è©¢
        if "database" in source_list:
            db_results = self.collect_from_database(topic, max_results)
            all_data.extend(db_results)

        # è¨ˆç®—è³ªé‡åˆ†æ•¸ä¸¦éæ¿¾
        scored_data = []
        for item in all_data:
            quality_score = self.calculate_quality_score(item)
            item["quality_score"] = quality_score

            if quality_score >= self.min_score:
                scored_data.append(item)

        # æŒ‰åˆ†æ•¸æ’åº
        scored_data.sort(key=lambda x: x["quality_score"], reverse=True)

        self.collected_data = scored_data

        # æå–é—œéµæ´å¯Ÿ
        key_insights = self.extract_key_insights(scored_data)

        print("\n" + "="*60)
        print("âœ… è³‡æ–™æ”¶é›†å®Œæˆ")
        print("="*60)

        return {
            "topic": topic,
            "total_items": len(scored_data),
            "min_quality_score": self.min_score,
            "data": scored_data,
            "key_insights": key_insights,
            "sources_used": source_list,
            "deep_research": deep_research
        }

    def save_to_file(self, research_data: Dict, filepath: str):
        """ä¿å­˜æ”¶é›†çš„è³‡æ–™åˆ°æ–‡ä»¶"""
        output = {
            "collected_at": datetime.now().isoformat(),
            **research_data
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"âœ… è³‡æ–™å·²ä¿å­˜åˆ°: {filepath}")

    def print_summary(self, research_data: Dict):
        """æ‰“å°æ”¶é›†æ‘˜è¦"""
        print(f"\nğŸ“Š æ”¶é›†æ‘˜è¦:")
        print(f"   ä¸»é¡Œ: {research_data['topic']}")
        print(f"   ç¸½å…±æ”¶é›†: {research_data['total_items']} é …")
        print(f"   æœ€ä½åˆ†æ•¸: {research_data['min_quality_score']}")
        print(f"   ä½¿ç”¨ä¾†æº: {', '.join(research_data['sources_used'])}")
        print(f"   æ·±åº¦ç ”ç©¶: {'æ˜¯' if research_data['deep_research'] else 'å¦'}")

        if research_data['total_items'] > 0:
            avg_score = sum(d["quality_score"] for d in research_data["data"]) / len(research_data["data"])
            print(f"   å¹³å‡åˆ†æ•¸: {avg_score:.2f}")

            print(f"\nğŸ“Œ é ‚ç´šè³‡æ–™ä¾†æº:")
            for i, item in enumerate(research_data["data"][:5], 1):
                print(f"   {i}. [{item['source_type']}] {item['title'][:60]}...")
                print(f"      åˆ†æ•¸: {item['quality_score']:.2f}")
                if item.get('url'):
                    print(f"      é€£çµ: {item['url']}")

        if research_data.get('key_insights'):
            print(f"\nğŸ’¡ é—œéµæ´å¯Ÿ:")
            for i, insight in enumerate(research_data['key_insights'][:3], 1):
                print(f"   {i}. {insight[:100]}...")


def main():
    parser = argparse.ArgumentParser(
        description="æ”¶é›†ç ”ç©¶è³‡æ–™ä¸¦è©•åˆ†ï¼ˆæ•´åˆ MCPï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # åŸºç¤æœå°‹
  python3 collect.py --topic "Moltbot" --sources web_search

  # æ·±åº¦ç ”ç©¶ï¼ˆæœå°‹ + è®€å–ç¶²é ï¼‰
  python3 collect.py --topic "Moltbot" --sources web_search,web_reader --deep-research

  # åŒ…å«è³‡æ–™åº«
  python3 collect.py --topic "AIå·¥å…·" --sources web_search,web_reader,database
        """
    )
    parser.add_argument("--topic", required=True, help="ç ”ç©¶ä¸»é¡Œ")
    parser.add_argument("--sources", default="web_search",
                       help="è³‡æ–™ä¾†æºï¼ˆweb_search, web_reader, databaseï¼‰")
    parser.add_argument("--max-results", type=int, default=20,
                       help="æœ€å¤§çµæœæ•¸é‡")
    parser.add_argument("--min-score", type=float, default=0.6,
                       help="æœ€å°è³ªé‡åˆ†æ•¸ (0-1)")
    parser.add_argument("--deep-research", action="store_true",
                       help="æ·±åº¦ç ”ç©¶ï¼šè®€å–ç¶²é å®Œæ•´å…§å®¹")
    parser.add_argument("--output", default="research_data.json",
                       help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()

    # å‰µå»ºæ”¶é›†å™¨
    collector = ResearchCollector(min_score=args.min_score)

    # æ”¶é›†è³‡æ–™
    research_data = collector.collect(
        topic=args.topic,
        sources=args.sources,
        max_results=args.max_results,
        deep_research=args.deep_research
    )

    # æ‰“å°æ‘˜è¦
    collector.print_summary(research_data)

    # ä¿å­˜åˆ°æ–‡ä»¶
    collector.save_to_file(research_data, args.output)


if __name__ == "__main__":
    main()
