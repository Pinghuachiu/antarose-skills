#!/usr/bin/env python3
"""
Social Content Writer - Content Analyzer
åˆ†æå…§å®¹è³ªé‡ä¸¦æä¾›æ”¹é€²å»ºè­°
"""

import os
import sys
import json
import argparse
from datetime import datetime
from typing import Dict, List
import re


# è³ªé‡è©•ä¼°æ¨™æº–
QUALITY_METRICS = {
    "length": {
        "weight": 0.15,
        "optimal_range": {
            "facebook": (300, 800),
            "instagram": (100, 200),
            "threads": (50, 200),
            "linkedin": (800, 1500)
        }
    },
    "structure": {
        "weight": 0.25,
        "criteria": ["has_hook", "has_body", "has_cta", "clear_flow"]
    },
    "engagement": {
        "weight": 0.20,
        "criteria": ["questions", "emotional_words", "action_verbs"]
    },
    "readability": {
        "weight": 0.15,
        "criteria": ["sentence_length", "paragraph_length", "jargon_usage"]
    },
    "hashtags": {
        "weight": 0.10,
        "optimal_count": {
            "facebook": (2, 5),
            "instagram": (15, 25),
            "threads": (2, 5),
            "linkedin": (2, 5)
        }
    },
    "emotional_impact": {
        "weight": 0.15,
        "criteria": ["power_words", "story_elements", "personal_touch"]
    }
}


class ContentAnalyzer:
    """å…§å®¹åˆ†æå™¨"""

    def __init__(self, platform: str = "facebook"):
        self.platform = platform
        self.metrics = QUALITY_METRICS

    def analyze(self, content: str, hashtags: List[str] = None) -> Dict:
        """å…¨é¢åˆ†æå…§å®¹"""
        if hashtags is None:
            hashtags = []

        results = {
            "platform": self.platform,
            "overall_score": 0,
            "metrics": {},
            "suggestions": [],
            "strengths": [],
            "improvements": []
        }

        # 1. é•·åº¦åˆ†æ
        length_score, length_analysis = self._analyze_length(content)
        results["metrics"]["length"] = length_analysis
        results["overall_score"] += length_score * self.metrics["length"]["weight"]

        # 2. çµæ§‹åˆ†æ
        structure_score, structure_analysis = self._analyze_structure(content)
        results["metrics"]["structure"] = structure_analysis
        results["overall_score"] += structure_score * self.metrics["structure"]["weight"]

        # 3. äº’å‹•æ€§åˆ†æ
        engagement_score, engagement_analysis = self._analyze_engagement(content)
        results["metrics"]["engagement"] = engagement_analysis
        results["overall_score"] += engagement_score * self.metrics["engagement"]["weight"]

        # 4. å¯è®€æ€§åˆ†æ
        readability_score, readability_analysis = self._analyze_readability(content)
        results["metrics"]["readability"] = readability_analysis
        results["overall_score"] += readability_score * self.metrics["readability"]["weight"]

        # 5. æ¨™ç±¤åˆ†æ
        hashtag_score, hashtag_analysis = self._analyze_hashtags(hashtags)
        results["metrics"]["hashtags"] = hashtag_analysis
        results["overall_score"] += hashtag_score * self.metrics["hashtags"]["weight"]

        # 6. æƒ…æ„Ÿå½±éŸ¿åŠ›åˆ†æ
        emotional_score, emotional_analysis = self._analyze_emotional_impact(content)
        results["metrics"]["emotional_impact"] = emotional_analysis
        results["overall_score"] += emotional_score * self.metrics["emotional_impact"]["weight"]

        # è½‰æ›ç‚ºç™¾åˆ†æ¯”
        results["overall_score"] = round(results["overall_score"] * 100, 1)

        # ç”Ÿæˆå»ºè­°
        results["suggestions"] = self._generate_suggestions(results)

        # è­˜åˆ¥å„ªå‹¢å’Œæ”¹é€²é»
        results["strengths"] = self._identify_strengths(results)
        results["improvements"] = self._identify_improvements(results)

        return results

    def _analyze_length(self, content: str) -> tuple:
        """åˆ†æå…§å®¹é•·åº¦"""
        length = len(content)
        optimal = self.metrics["length"]["optimal_range"][self.platform]
        min_opt, max_opt = optimal

        if min_opt <= length <= max_opt:
            score = 1.0
            status = "ç†æƒ³"
        elif length < min_opt:
            score = max(0.3, length / min_opt)
            status = "åçŸ­"
        else:
            score = max(0.5, max_opt / length)
            status = "åé•·"

        analysis = {
            "length": length,
            "optimal_range": optimal,
            "status": status,
            "score": round(score, 2)
        }

        return score, analysis

    def _analyze_structure(self, content: str) -> tuple:
        """åˆ†æå…§å®¹çµæ§‹"""
        criteria = self.metrics["structure"]["criteria"]
        scores = {}

        # æª¢æŸ¥æ˜¯å¦æœ‰å‹¾å­ï¼ˆé–‹é ­å•å¥æˆ–æ„Ÿå˜†å¥ï¼‰
        has_hook = bool(re.search(r'^.{0,50}[?ï¼]', content))
        scores["has_hook"] = 1.0 if has_hook else 0.3

        # æª¢æŸ¥æ˜¯å¦æœ‰æ­£æ–‡ï¼ˆå…§å®¹æ˜¯å¦è¶³å¤ é•·ï¼‰
        has_body = len(content) > 100
        scores["has_body"] = 1.0 if has_body else 0.5

        # æª¢æŸ¥æ˜¯å¦æœ‰è¡Œå‹•å¬å–š
        cta_keywords = ["ç•™è¨€", "åˆ†äº«", "é—œæ³¨", "é»æ“Š", "comment", "share", "follow"]
        has_cta = any(keyword in content for keyword in cta_keywords)
        scores["has_cta"] = 1.0 if has_cta else 0.4

        # æª¢æŸ¥æµç¨‹ï¼ˆæ˜¯å¦åˆ†æ®µæ¸…æ™°ï¼‰
        clear_flow = content.count('\n\n') >= 1 or len(re.findall(r'[ã€‚ï¼ï¼Ÿ]', content)) >= 3
        scores["clear_flow"] = 1.0 if clear_flow else 0.6

        avg_score = sum(scores.values()) / len(scores)

        analysis = {
            "criteria": scores,
            "score": round(avg_score, 2),
            "details": {
                "has_hook": has_hook,
                "has_body": has_body,
                "has_cta": has_cta,
                "clear_flow": clear_flow
            }
        }

        return avg_score, analysis

    def _analyze_engagement(self, content: str) -> tuple:
        """åˆ†æäº’å‹•æ€§"""
        # æª¢æŸ¥å•å¥
        questions = len(re.findall(r'[?ï¼Ÿ]', content))
        question_score = min(1.0, questions / 2)

        # æª¢æŸ¥æƒ…æ„Ÿè©
        emotional_words = ["é©šå–œ", "èˆˆå¥®", "å–œæ­¡", "æ„›", "é–‹å¿ƒ", "amazing", "love", "excited"]
        emotional_count = sum(1 for word in emotional_words if word in content.lower())
        emotional_score = min(1.0, emotional_count / 2)

        # æª¢æŸ¥è¡Œå‹•å‹•è©
        action_verbs = ["ç«‹å³", "ç¾åœ¨", "è¶•å¿«", "é–‹å§‹", "start", "now", "discover"]
        action_count = sum(1 for verb in action_verbs if verb in content.lower())
        action_score = min(1.0, action_count / 2)

        avg_score = (question_score + emotional_score + action_score) / 3

        analysis = {
            "questions": questions,
            "emotional_words": emotional_count,
            "action_verbs": action_count,
            "score": round(avg_score, 2)
        }

        return avg_score, analysis

    def _analyze_readability(self, content: str) -> tuple:
        """åˆ†æå¯è®€æ€§"""
        # å¹³å‡å¥å­é•·åº¦
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0

        # å¥å­é•·åº¦è©•åˆ†
        if avg_sentence_length < 30:
            sentence_score = 1.0
        elif avg_sentence_length < 50:
            sentence_score = 0.8
        else:
            sentence_score = 0.6

        # æ®µè½é•·åº¦
        paragraphs = content.split('\n\n')
        avg_paragraph_length = sum(len(p) for p in paragraphs) / len(paragraphs) if paragraphs else 0

        # æ®µè½é•·åº¦è©•åˆ†
        if avg_paragraph_length < 200:
            paragraph_score = 1.0
        elif avg_paragraph_length < 400:
            paragraph_score = 0.8
        else:
            paragraph_score = 0.6

        avg_score = (sentence_score + paragraph_score) / 2

        analysis = {
            "avg_sentence_length": round(avg_sentence_length, 1),
            "avg_paragraph_length": round(avg_paragraph_length, 1),
            "score": round(avg_score, 2)
        }

        return avg_score, analysis

    def _analyze_hashtags(self, hashtags: List[str]) -> tuple:
        """åˆ†ææ¨™ç±¤"""
        count = len(hashtags)
        optimal = self.metrics["hashtags"]["optimal_count"][self.platform]
        min_opt, max_opt = optimal

        if min_opt <= count <= max_opt:
            score = 1.0
            status = "ç†æƒ³"
        elif count < min_opt:
            score = max(0.5, count / min_opt)
            status = "åå°‘"
        else:
            score = max(0.5, max_opt / count)
            status = "åå¤š"

        analysis = {
            "count": count,
            "hashtags": hashtags,
            "optimal_range": optimal,
            "status": status,
            "score": round(score, 2)
        }

        return score, analysis

    def _analyze_emotional_impact(self, content: str) -> tuple:
        """åˆ†ææƒ…æ„Ÿå½±éŸ¿åŠ›"""
        # å¼·åŠ›è©å½™
        power_words = [
            "é©å‘½æ€§", "çªç ´", "é©šäºº", "çµ•ä½³", "å¿…é ˆ", "revolutionary",
            "breakthrough", "amazing", "must-have", "essential"
        ]
        power_count = sum(1 for word in power_words if word in content.lower())
        power_score = min(1.0, power_count / 3)

        # æ•…äº‹å…ƒç´ 
        story_indicators = ["ç•¶æˆ‘", "æˆ‘æ›¾ç¶“", "ç¶“æ­·", "æ•…äº‹", "when I", "my story", "experience"]
        story_score = 1.0 if any(indicator in content for indicator in story_indicators) else 0.6

        # å€‹äººè§¸æ‘¸
        personal_indicators = ["æˆ‘", "æˆ‘çš„", "I", "my", "me"]
        personal_count = sum(content.lower().count(indicator) for indicator in personal_indicators)
        personal_score = min(1.0, personal_count / 10)

        avg_score = (power_score + story_score + personal_score) / 3

        analysis = {
            "power_words": power_count,
            "story_elements": story_score,
            "personal_touch": personal_count,
            "score": round(avg_score, 2)
        }

        return avg_score, analysis

    def _generate_suggestions(self, results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        suggestions = []

        # é•·åº¦å»ºè­°
        length_status = results["metrics"]["length"]["status"]
        if length_status == "åçŸ­":
            suggestions.append("å…§å®¹åçŸ­ï¼Œå»ºè­°å¢åŠ æ›´å¤šç´°ç¯€å’Œä¾‹å­")
        elif length_status == "åé•·":
            suggestions.append("å…§å®¹åé•·ï¼Œè€ƒæ…®åˆ†æ®µæˆ–ç¸®æ¸›éƒ¨åˆ†å…§å®¹")

        # çµæ§‹å»ºè­°
        if not results["metrics"]["structure"]["details"]["has_hook"]:
            suggestions.append("ç¼ºå°‘å¸å¼•äººçš„é–‹é ­å‹¾å­ï¼Œå»ºè­°åŠ å…¥å•å¥æˆ–é©šäººäº‹å¯¦")

        if not results["metrics"]["structure"]["details"]["has_cta"]:
            suggestions.append("ç¼ºå°‘æ˜ç¢ºçš„è¡Œå‹•å¬å–šï¼Œå»ºè­°åœ¨çµå°¾åŠ å…¥ã€Œç•™è¨€åˆ†äº«ã€ç­‰æç¤º")

        # äº’å‹•æ€§å»ºè­°
        if results["metrics"]["engagement"]["questions"] < 2:
            suggestions.append("äº’å‹•æ€§ä¸è¶³ï¼Œå»ºè­°å¢åŠ å•å¥ä»¥å¼•ç™¼è¨è«–")

        # æ¨™ç±¤å»ºè­°
        hashtag_status = results["metrics"]["hashtags"]["status"]
        if hashtag_status == "åå°‘":
            suggestions.append(f"æ¨™ç±¤åå°‘ï¼Œå»ºè­°å¢åŠ åˆ° {self.metrics['hashtags']['optimal_count'][self.platform][0]} å€‹ä»¥ä¸Š")
        elif hashtag_status == "åå¤š":
            suggestions.append("æ¨™ç±¤éå¤šå¯èƒ½é¡¯å¾—åƒåœ¾ï¼Œè€ƒæ…®åªä¿ç•™æœ€ç›¸é—œçš„")

        # æƒ…æ„Ÿå½±éŸ¿åŠ›å»ºè­°
        if results["metrics"]["emotional_impact"]["power_words"] < 2:
            suggestions.append("å¯ä»¥åŠ å…¥æ›´å¤šå¼·åŠ›è©å½™å¢å¼·æƒ…æ„Ÿè¡æ“Š")

        if not results["metrics"]["emotional_impact"]["story_elements"]:
            suggestions.append("è€ƒæ…®åŠ å…¥å€‹äººæ•…äº‹æˆ–æ¡ˆä¾‹å¢åŠ çœŸå¯¦æ„Ÿ")

        return suggestions

    def _identify_strengths(self, results: Dict) -> List[str]:
        """è­˜åˆ¥å…§å®¹å„ªå‹¢"""
        strengths = []

        if results["metrics"]["length"]["status"] == "ç†æƒ³":
            strengths.append("å…§å®¹é•·åº¦é©ä¸­")

        if results["metrics"]["structure"]["details"]["has_hook"]:
            strengths.append("æœ‰å¸å¼•äººçš„é–‹é ­å‹¾å­")

        if results["metrics"]["structure"]["details"]["has_cta"]:
            strengths.append("åŒ…å«æ˜ç¢ºçš„è¡Œå‹•å¬å–š")

        if results["metrics"]["engagement"]["questions"] >= 2:
            strengths.append("äº’å‹•æ€§è‰¯å¥½ï¼ˆåŒ…å«å¤šå€‹å•å¥ï¼‰")

        if results["metrics"]["hashtags"]["status"] == "ç†æƒ³":
            strengths.append("æ¨™ç±¤ä½¿ç”¨æ°ç•¶")

        if results["metrics"]["emotional_impact"]["story_elements"]:
            strengths.append("åŒ…å«æ•…äº‹å…ƒç´ å¢å¼·å¸å¼•åŠ›")

        return strengths

    def _identify_improvements(self, results: Dict) -> List[str]:
        """è­˜åˆ¥éœ€è¦æ”¹é€²çš„åœ°æ–¹"""
        improvements = []

        if results["metrics"]["structure"]["score"] < 0.7:
            improvements.append("å…§å®¹çµæ§‹éœ€è¦å„ªåŒ–")

        if results["metrics"]["engagement"]["score"] < 0.6:
            improvements.append("äº’å‹•æ€§éœ€è¦æå‡")

        if results["metrics"]["readability"]["score"] < 0.7:
            improvements.append("å¯è®€æ€§éœ€è¦æ”¹å–„")

        if results["metrics"]["emotional_impact"]["score"] < 0.6:
            improvements.append("æƒ…æ„Ÿå½±éŸ¿åŠ›ä¸è¶³")

        return improvements

    def print_analysis(self, results: Dict):
        """æ‰“å°åˆ†æçµæœ"""
        print("\n" + "="*60)
        print(f"ğŸ“Š å…§å®¹åˆ†æå ±å‘Š ({results['platform'].upper()})")
        print("="*60)

        print(f"\nç¸½é«”è©•åˆ†: {results['overall_score']}/100")

        print("\nğŸ“ˆ å„é …æŒ‡æ¨™:")
        print("-" * 60)

        for metric_name, metric_data in results["metrics"].items():
            score = metric_data.get("score", 0)
            score_bar = "â–ˆ" * int(score * 10) + "â–‘" * (10 - int(score * 10))
            print(f"\n{metric_name.upper().replace('_', ' ')}")
            print(f"  åˆ†æ•¸: {score*100:.0f}/100  [{score_bar}]")

            # é¡¯ç¤ºè©³ç´°ä¿¡æ¯
            if metric_name == "length":
                print(f"  é•·åº¦: {metric_data['length']:,} å­—")
                print(f"  ç‹€æ…‹: {metric_data['status']}")
            elif metric_name == "structure":
                details = metric_data.get("details", {})
                if details.get("has_hook"):
                    print(f"  âœ“ åŒ…å«å‹¾å­")
                if details.get("has_cta"):
                    print(f"  âœ“ åŒ…å«è¡Œå‹•å¬å–š")
            elif metric_name == "engagement":
                print(f"  å•å¥æ•¸: {metric_data['questions']}")
                print(f"  æƒ…æ„Ÿè©: {metric_data['emotional_words']}")
            elif metric_name == "hashtags":
                print(f"  æ¨™ç±¤æ•¸: {metric_data['count']} ({metric_data['status']})")

        # å„ªå‹¢
        if results["strengths"]:
            print(f"\nâœ… å„ªå‹¢:")
            for strength in results["strengths"]:
                print(f"   â€¢ {strength}")

        # æ”¹é€²å»ºè­°
        if results["improvements"]:
            print(f"\nâš ï¸  éœ€è¦æ”¹é€²:")
            for improvement in results["improvements"]:
                print(f"   â€¢ {improvement}")

        # å…·é«”å»ºè­°
        if results["suggestions"]:
            print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
            for i, suggestion in enumerate(results["suggestions"], 1):
                print(f"   {i}. {suggestion}")

        print("="*60 + "\n")

    def save_to_file(self, results: Dict, filepath: str):
        """ä¿å­˜åˆ†æçµæœåˆ°æ–‡ä»¶"""
        output = {
            "analyzed_at": datetime.now().isoformat(),
            **results
        }

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)

        print(f"âœ… åˆ†æçµæœå·²ä¿å­˜åˆ°: {filepath}")


def main():
    parser = argparse.ArgumentParser(
        description="åˆ†æå…§å®¹è³ªé‡ä¸¦æä¾›æ”¹é€²å»ºè­°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¯„ä¾‹ï¼š
  # åˆ†æå…§å®¹æ–‡ä»¶
  python3 analyze.py --content content.json --platform facebook

  # åˆ†ææ–‡å­—å…§å®¹
  python3 analyze.py --content "ä½ çš„å…§å®¹æ–‡å­—" --platform instagram --detailed

  # ä½¿ç”¨äº¤äº’æ¨¡å¼
  python3 analyze.py --interactive
        """
    )
    parser.add_argument("--content", help="å…§å®¹æª”æ¡ˆè·¯å¾‘æˆ–å…§å®¹æ–‡å­—")
    parser.add_argument("--platform", default="facebook",
                       choices=["facebook", "instagram", "threads", "linkedin"],
                       help="ç›®æ¨™å¹³å°")
    parser.add_argument("--hashtags", help="æ¨™ç±¤åˆ—è¡¨ï¼ˆé€—è™Ÿåˆ†éš”ï¼‰")
    parser.add_argument("--detailed", action="store_true",
                       help="é¡¯ç¤ºè©³ç´°åˆ†æ")
    parser.add_argument("--output", default="analysis_results.json",
                       help="è¼¸å‡ºæª”æ¡ˆè·¯å¾‘")

    args = parser.parse_args()

    if not args.content:
        # äº¤äº’æ¨¡å¼
        print("ğŸ“ è«‹è¼¸å…¥è¦åˆ†æçš„å†…å®¹ï¼ˆæŒ‰ Ctrl+D çµæŸè¼¸å…¥ï¼‰:")
        content_lines = []
        try:
            for line in sys.stdin:
                content_lines.append(line)
        except KeyboardInterrupt:
            pass
        content = "".join(content_lines)
        hashtags = []
    elif os.path.exists(args.content):
        # å¾æ–‡ä»¶è®€å–
        with open(args.content, 'r', encoding='utf-8') as f:
            data = json.load(f)
            content = data.get("content", "")
            hashtags = data.get("hashtags", [])
    else:
        # ç›´æ¥ä½¿ç”¨è¼¸å…¥çš„æ–‡å­—
        content = args.content
        hashtags = args.hashtags.split(",") if args.hashtags else []

    if not content:
        print("âŒ éŒ¯èª¤ï¼šå…§å®¹ç‚ºç©º")
        return

    # å‰µå»ºåˆ†æå™¨
    analyzer = ContentAnalyzer(platform=args.platform)

    # åˆ†æå…§å®¹
    results = analyzer.analyze(content, hashtags)

    # æ‰“å°çµæœ
    analyzer.print_analysis(results)

    # ä¿å­˜åˆ°æ–‡ä»¶
    analyzer.save_to_file(results, args.output)


if __name__ == "__main__":
    main()
